#!/usr/bin/env python3

# Run benchmarks and compare the results interactively.

import argparse
import json
import math
import os
import os.path
import platform
import re
import signal
import subprocess
import sys
import time
import tempfile

LibDir = os.path.join(os.path.dirname(__file__), 'lib')
sys.path.insert(0, LibDir)

from build import Build
from format import *
from perf import *
from stats import *
from test import *
import display
import gcprofile

def main():
    args = parseArgs()
    builds = buildsToTest(args)
    tests = testsToRun(args)
    runTests(args, builds, tests)

def parseArgs():
    parser = argparse.ArgumentParser(
        description = 'Run benchmarks and compare the results interactively')
    parser.add_argument('--args', default='', help='Arguments passed to every build')
    parser.add_argument('-t', '--test', help='Test suite to run')
    parser.add_argument('--iterations', type=int, default=200,
                        help='The number of times to run each test')
    parser.add_argument('--show-samples', action='store_true')
    parser.add_argument('--gc-profile', action='store_true')
    parser.add_argument('--sys-usage', action='store_true')
    parser.add_argument('--perf', action='store_true')
    parser.add_argument('--geomean', action='store_true')
    parser.add_argument('--output', '-o', help='Write results to file in JSON format')
    parser.add_argument('--numa', action='store_true', help='Bind CPU and memory to NUMA node 1')
    parser.add_argument('builds', nargs="+")
    return parser.parse_args()

def buildsToTest(args):
    return list(map(Build, args.builds))

def testsToRun(args):
    allTests = getKnownTests()

    if not args.test:
        return [allTests[0]]

    tests = list(filter(lambda t: t.name == args.test, allTests))
    if tests:
        return tests

    return [LocalTest(args.test)]

def getKnownTests():
    return [
        # Run all tests sequentially in a single runtime.
        OctaneTest('octane', 'run.js'),

        # Run individual tests independently.
        OctaneTest('richards'),
        OctaneTest('deltablue'),
        OctaneTest('crypto'),
        OctaneTest('raytrace'),
        OctaneTest('earley-boyer'),
        OctaneTest('regexp'),
        OctaneTest('splay'),
        OctaneTest('navier-stokes'),
        OctaneTest('pdfjs'),
        OctaneTest('mandreel'),
        OctaneTest('gbemu'),
        OctaneTest('code-load'),
        OctaneTest('box2d'),
        OctaneTest('zlib'),
        OctaneTest('typescript')
    ]

def runTests(args, builds, tests):
    # Nested map of lists keyed by result key then by build.
    results = dict()

    headerWidth = printHeader()

    out = None
    if sys.stdout.isatty():
        out = display.Terminal()

    for i in range(args.iterations):
        for build in builds:
            for test in tests:
                bmResults = runBenchmark(build, test, args)
                for key in bmResults.keys():
                    addResult(builds, results, build, key, bmResults[key])
                if out:
                    with DelayedKeyboardInterrupt():
                        displayResults(out, builds, results, args, headerWidth)

        if args.output:
            with DelayedKeyboardInterrupt():
                writeResultsToFile(builds, results, args)

    if not sys.stdout.isatty():
        out = display.File(sys.stdout)
        displayResults(out, builds, results, args, headerWidth)

class DelayedKeyboardInterrupt(object):
    # From https://stackoverflow.com/a/21919644
    def __enter__(self):
        self.signal_received = False
        self.old_handler = signal.signal(signal.SIGINT, self.handler)

    def handler(self, sig, frame):
        self.signal_received = (sig, frame)

    def __exit__(self, type, value, traceback):
        signal.signal(signal.SIGINT, self.old_handler)
        if self.signal_received:
            self.old_handler(*self.signal_received)

def runBenchmark(build, test, args):
    oldcwd = os.getcwd()
    os.chdir(test.dir)
    cmd = [build.shell] + build.args + args.args.split() + [test.script] + test.args
    env = dict()

    profilePath = None
    if args.gc_profile:
        temp = tempfile.NamedTemporaryFile(delete=False)
        temp.close()
        profilePath = temp.name
        env['JS_GC_PROFILE'] = '0'
        env['JS_GC_PROFILE_NURSERY'] = '0'
        env['JS_GC_PROFILE_FILE'] = profilePath

    if args.numa:
        cmd = ['numactl', '--cpunodebind=1', '--localalloc', '--'] + cmd

    if args.sys_usage:
        flag = "-l" if platform.system() == "Darwin" else "-v"
        cmd = ['/usr/bin/time', flag] + cmd
    elif args.perf:
        cmd = updateCommandForPerf(args, cmd)

    time.sleep(0.2)

    proc = subprocess.run(cmd, env=env, capture_output=True, text=True)

    if proc.returncode != 0:
        print(f"Error running benchmark {test.name} with shell {build.shell}:")
        print(' '.join(cmd))
        print(f"Command exited with return code {proc.returncode}")
        print(proc.stderr)
        sys.exit(1)
    os.chdir(oldcwd)

    return parseOutput(proc.stdout, proc.stderr, args, profilePath)

def addResult(builds, results, build, key, result):
    if key not in results:
        results[key] = dict()
        for b in builds:
            results[key][b] = []

    results[key][build].append(result)

def parseOutput(stdout, stderr, args, profilePath):
    result = dict()
    for line in stdout.splitlines():
        match = re.match(r'(\w+):\s(\d+(:?\.\d+)?)', line)
        if match:
            key, value = match.group(1), match.group(2)
            result['!' + key] = float(value)

    if profilePath:
        with open(profilePath) as f:
            gcprofile.summariseProfile(f.read(), result, False)
        os.remove(profilePath)

    if args.sys_usage:
        parseSysUsage(result, stderr)

    if args.perf:
        parsePerfOutput(result, stderr)

    if not result:
        print(stdout + stderr)
        sys.exit("Can't parse output")

    return result

def parseSysUsage(result, text):
    for line in text.splitlines():
        if platform.system() == "Darwin":
            parts = line.strip().split(" ")
            value = parts[0]
            key = " ".join(parts[1:])
        else:
            key, value = line.strip().split(": ")

        if value.endswith("%"):
            value = int(value[:-1])
        elif ":" in value:
            hours, minsAndSecs = value.split(":")
            value = int(hours) * 60 + float(minsAndSecs)
        elif re.match(r"\d+(\.\d+)?$", value):
            value = float(value)
        else:
            continue

        if platform.system() == "Darwin" and "size" in key:
            value = value / 1024
            key += " (KB)"

        result[key] = value

def printHeader():
    header = (24 * " ") + statsHeader()
    width = len(header)
    print(header)
    print(width * "=")
    return width

def displayResults(out, builds, results, args, headerWidth):
    out.clear()

    if args.geomean:
        count = 0
        geomean = dict()
        for build in builds:
            geomean[build] = 0

    for key in results.keys():
        statsForBuild = dict()
        compareTo = None
        low = None
        high = None
        first = True
        for build in builds:
            data = results[key][build]
            if not data:
                continue

            stats = Stats(data)
            if stats.min == 0 and stats.max == 0:
                continue

            statsForBuild[build] = stats
            if args.geomean and key.startswith('!') and stats.mean != 0:
                geomean[build] += math.log(stats.mean)
            if first:
                compareTo = stats
                low, high = stats.min, stats.max
                first = False
            else:
                low, high = min(low, stats.min), max(high, stats.max)

        if not statsForBuild:
            continue

        if low == high:
            continue

        if key.startswith('!'):
            key = key[1:]
            if args.geomean:
                count += 1

        out.print(f"{key}:")
        for build in statsForBuild.keys():
            stats = statsForBuild[build]
            comp = compareStats(stats, compareTo)
            text = formatStats(stats, comp)

            if stats.count > 1 and low != high:
                text += "  " + formatBox(low, high, stats)
                if args.show_samples:
                    text += "  " + formatSamples(low, high, stats)

            out.print("  %20s  %s" % (build.spec[-20:], text))

    if args.geomean and count > 1 and not args.gc_profile:
        out.print()
        out.print("Geometric mean:")
        compareTo = None
        first = True
        for build in builds:
            if geomean[build] != 0:
                mean = math.exp(geomean[build] / count)
                text = "          %8s" % formatFloat(8, mean)
                if first:
                    compareTo = mean
                    first = False
                else:
                    diff = (mean - compareTo) / compareTo
                    text += "                                     %4.1f%%" % (diff * 100)
                out.print("  %20s  %s" % (build.spec[-20:], text))

def writeResultsToFile(builds, results, args):
    data = []
    for build in builds:
        buildData = {
            'build': build.spec,
            'system': platform.system(),
            'architecture': platform.machine(),
            'results': dict()
        }

        for key in results.keys():
            if build in results[key]:
                stats = Stats(results[key][build])
                if key.startswith('!'):
                    key = key[1:]
                buildData['results'][key] = stats.__dict__

        data.append(buildData)

    with open(args.output, "w") as f:
        json.dump(data, f, allow_nan=False, indent=2)

try:
    main()
except KeyboardInterrupt:
    pass
